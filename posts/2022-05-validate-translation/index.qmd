---
title: "Validate model translation from NONMEM"
author: Kyle Baron
date: 05/25/2022
description: > 
  If your mrgsolve model is a translation of a completed NONMEM model run, you 
  can validate it every time; find out how.
categories: 
- model specification
- validation
twitter-card: 
  image: "images/hex.png"
---

# Introduction

We frequently use mrgsolve to simulate from models estimated in NONMEM. This 
requires _translation_ of the model from the NONMEM control stream into 
mrgsolve format. 

There is a straightforward way to confirm correct coding of the mrgsolve model
once the NONMEM model is in hand. This blog post will show you how to do that.


:::{.callout-note collapse=true appearance="simple"}

## Expand view packages and options

```{r}
#| message: false
#| warning: false
library(mrgsolve)
library(dplyr)
library(tidyr)
library(data.table)
library(here)
options(bbr.verbose = FALSE, mrgsolve.project = here("model/pk"))
```

:::

# Data

The validation of the mrgsolve model coding is driven by `PRED` which was 
tabled when the NONMEM run finished. This is the standard against which 
we will evaluate the mrgsolve model. 

```{r}
#| message: false
#| warning: false
tab <- fread(here("model/pk/106/106.tab"))
head(tab)
```

We want to make sure we have the _same input data_ for the validation that 
we had for the model estimation.  So we will read in the analysis data

```{r}
#| message: false
#| warning: false
nmdata <- fread(here("data/derived/analysis3.csv"), na.strings = '.')
nmdata <- select(nmdata, -DV)
```

and then merge it on by a row counter called `NUM` which uniquely connects 
output rows to input rows

```{r}
data <- left_join(tab, nmdata, by = "NUM")
```

Note that in the join, `tab` is on the left so that the resulting `data` drops
the records from `nmdata` that didn't make it into the analysis.


Now, we have the analysis data set with `PRED` (from NONMEM) as a column

```{r}
select(data, ID, NUM, TIME, EVID, AMT, CMT, DV, PRED)
```

## bbr::nm_join

::: {.callout-note appearance="simple"}

If you are using [bbr]({{< var links.bbr >}})  as your modeling platform, you
can get the same data set with a one-liner call to `bbr::nm_join()`



```{r, eval = FALSE}
data <- nm_join(here("model/pk/106"))
```


:::


# Model

The model is a standard two-compartment PK model from run `106`; we've already 
coded the model here

```{r}
#| message: false
#| warning: false
mod <- mread_cache("106.txt")
```

:::{.callout-note collapse=true appearance="simple"}

## Expand to see the model code

```{c, code = mod@code}
#| eval: false
```

:::

# Simulate

Now, simulate from the mrgsolve model

```{r}
out <- mrgsim(zero_re(mod), data, carry_out = "PRED,EVID", digits = 7)
head(out)
```

Some important points to notice about this simulation

1. We wrap the model object in `zero_re()`; this drops all the random effects 
   from the simulation so that simulated concentrations are equivalent to `PRED`
   regardless if we are looking at `DV` (or `Y`) or `IPRED`
1. We bring `PRED` and `EVID` into the simulated output; we need `PRED` to 
   to compare against and `EVID` is needed so we can filter out dosing records
1. We deliberately set the number of significant `digits` in the simulated 
   result; this is really important if you want a sensitive validation index



# Compare


Keeping only the observation records, we make a simple summary of the 
difference between the prediction generated by mrgsolve (`Y`) and the value
generated by NONMEM (`PRED`)

```{r}
obs <- filter(out,  EVID==0)
summary(obs$Y - obs$PRED)
```

We see there is exact match up to the level of precision that we've
requested (See [Appendix](2022-05-validate-translation.html#sec-appendix)). 

This can be presented graphically as well

```{r}
plot(obs$Y, obs$PRED)
```

Here we've looked at absolute differences between mrgsolve and 
NONMEM; if you do find a discrepancy, you might evaluate that as a percentage
of the value of `PRED` (percent difference).

::: {.callout-important}

## Important

We frequently see exact matches when models are coded with analytical solutions.
This is _never_ the case when coding models from ODEs. But even with ODE models
you should expect to see only a small discrepancy ... only a small fraction of 
one percent or less. 

:::


# Complete workflow

We've illustrated this step by step, explaining how it works. But wanted also 
to show you how little code is required once you have the NONMEM and mrgsolve
models.

```{r}
#| message: false
mod <- mread_cache("106.txt")
data <- bbr::nm_join(here("model/pk/106"))
out <- mrgsim(zero_re(mod), data, carry_out = "PRED,EVID", digits = 7)
obs <- filter(out,  EVID==0)
summary(obs$Y - obs$PRED)
```

We have this done in 5 lines and objects generated from 2 of those lines will be
used to run the production simulation.


# Discussion 

It is important to note that this validation procedure only considers model 
`PRED` given a clinical data set. This setup evaluates dosing events, covariate
effects, structural model setup and the like. 

Note that this setup _does not_ look at whether or not the random effects are 
coded correctly. Certainly there can be mistakes in coding the random effects, 
but in my experience, most errors in translation are related to input data sets, 
covariate models or errors with the structural model (e.g. coding the ODE 
correctly). 

We also note how critical it is to consider the number of digits and tolerances
of ODE solvers when comparing model outputs; when digits and tolerances are 
ignored, any discrepancy can seem bigger than it should be. 

Finally, we include a reminder that mrgsolve is not an exact clone of NONMEM:
we don't and can't match every behavior. So if you do find a discrepancy in your
validation, it _could_ be that you've tapped some functionality that is not 
consistent between NONMEM and mrgsolve. That said ... I've been using mrgsolve 
with NONMEM across many years of project work and the vast majority of models
we've worked with have matched when we are very careful in the translation and 
in carrying out the validation. 

If you are unable to validate your model, please check the code. Then check it 
again. If you're _sure_ the model coding is equivalent and you are still seeing
discrepancy, we encourage you to open ticket on our GitHub issue tracker
[here](https://github.com/metrumresearchgroup/mrgsolve/issues) so we can learn 
more about the discrepancy. 

And as always, be sure to visit [mrgsolve.org](https://mrgsolve.or) for more 
resources. 


# Appendix {#sec-appendix .appendix}

See what happens when we don't **control digits** on the simulation 

```{r}
#| message: false
mod <- mread_cache("106.txt")
data <- bbr::nm_join(here("model/pk/106"))
out <- mrgsim(zero_re(mod), data, carry_out = "PRED,EVID")
obs <- filter(out,  EVID==0)
summary(obs$Y - obs$PRED)
```

Here, we've used analytical solutions from both mrgsolve and NONMEM. This 
sort of error is what you might see when comparing two models which require
ODEs to generate the solution. 

This error is still very small relative to `PRED`

```{r}
summary(100*(obs$Y - obs$PRED)/obs$PRED)
```


Next, let's try to **validate a model that was coded incorrectly**. For the 
`AGE` effect on `CL`, we have the reference to be 45 years old but it should 
be 35. 

```{r}
#| include: false
code <- mod@code
code <- sub("AGE/35", "AGE/45", code, fixed = TRUE)
```


:::{.callout-note collapse=true appearance="simple"}

## Expand to see the model code



```{r}
#| message: false
mod2 <- mcode("bad", paste0(code, collapse = "\n"))
```

```{c, code = capture.output(blocks(mod2, main))}
#| eval: false

```

:::


When we try to validate this model against the NONMEM run with different 
reference `AGE`, we can clearly see there's an issue

```{r}
#| message: false
out <- mrgsim(zero_re(mod2), data, carry_out = "PRED,EVID", digits = 7)
obs <- filter(out,  EVID==0)
summary(obs$Y - obs$PRED)
```

